From 1091ed3a2636ef0c4dc3c0e4a23cfd3f068b4564 Mon Sep 17 00:00:00 2001
From: Alexander Shabalin <shabalyn.a@gmail.com>
Date: Tue, 23 Apr 2013 18:59:15 +0400
Subject: [PATCH] IO benchmarking - traces

---
 fs/bio.c                                      |  2 +
 fs/buffer.c                                   |  2 +
 fs/mpage.c                                    |  6 +++
 fs/read_write.c                               |  8 ++++
 include/trace/events/io-bench-custom-traces.h |  2 +
 include/trace/events/io-bench.h               | 57 +++++++++++++++++++++++++++
 kernel/trace/Kconfig                          |  4 ++
 kernel/trace/Makefile                         |  3 ++
 kernel/trace/io-bench.c                       | 18 +++++++++
 mm/filemap.c                                  | 32 +++++++++++++++
 mm/readahead.c                                |  3 ++
 11 files changed, 137 insertions(+)
 create mode 100644 include/trace/events/io-bench-custom-traces.h
 create mode 100644 include/trace/events/io-bench.h
 create mode 100644 kernel/trace/io-bench.c

diff --git a/fs/bio.c b/fs/bio.c
index b96fc6c..c300636 100644
--- a/fs/bio.c
+++ b/fs/bio.c
@@ -30,6 +30,7 @@
 #include <scsi/sg.h>		/* for struct sg_iovec */
 
 #include <trace/events/block.h>
+#include <trace/events/io-bench.h>
 
 /*
  * Test patch to inline a certain number of bi_io_vec's inside the bio
@@ -648,6 +649,7 @@ int bio_add_page(struct bio *bio, struct page *page, unsigned int len,
 		 unsigned int offset)
 {
 	struct request_queue *q = bdev_get_queue(bio->bi_bdev);
+   TRACING_TAP(read, "bio_add_page");
 	return __bio_add_page(q, bio, page, len, offset, queue_max_sectors(q));
 }
 EXPORT_SYMBOL(bio_add_page);
diff --git a/fs/buffer.c b/fs/buffer.c
index 20c0aae..6eea0db 100644
--- a/fs/buffer.c
+++ b/fs/buffer.c
@@ -41,6 +41,7 @@
 #include <linux/bitops.h>
 #include <linux/mpage.h>
 #include <linux/bit_spinlock.h>
+#include <trace/events/io-bench.h>
 
 static int fsync_buffers_list(spinlock_t *lock, struct list_head *list);
 
@@ -2102,6 +2103,7 @@ int block_read_full_page(struct page *page, get_block_t *get_block)
 	unsigned int blocksize, bbits;
 	int nr, i;
 	int fully_mapped = 1;
+   TRACING_TAP(read, "block_read_full_page");
 
 	head = create_page_buffers(page, inode, 0);
 	blocksize = head->b_size;
diff --git a/fs/mpage.c b/fs/mpage.c
index 0face1c..8986eb0 100644
--- a/fs/mpage.c
+++ b/fs/mpage.c
@@ -28,6 +28,7 @@
 #include <linux/backing-dev.h>
 #include <linux/pagevec.h>
 #include <linux/cleancache.h>
+#include <trace/events/io-bench.h>
 
 /*
  * I/O completion handler for multipage BIOs.
@@ -74,6 +75,7 @@ static void mpage_end_io(struct bio *bio, int err)
 static struct bio *mpage_bio_submit(int rw, struct bio *bio)
 {
 	bio->bi_end_io = mpage_end_io;
+   TRACING_TAP(read, "mpage_bio_submit");
 	submit_bio(rw, bio);
 	return NULL;
 }
@@ -171,6 +173,7 @@ do_mpage_readpage(struct bio *bio, struct page *page, unsigned nr_pages,
 	int fully_mapped = 1;
 	unsigned nblocks;
 	unsigned relative_block;
+   TRACING_TAP(read, "do_mpage_readpage start");
 
 	if (page_has_buffers(page))
 		goto confused;
@@ -285,6 +288,7 @@ do_mpage_readpage(struct bio *bio, struct page *page, unsigned nr_pages,
 		bio = mpage_bio_submit(READ, bio);
 
 alloc_new:
+   TRACING_TAP(read, "do_mpage_readpage alloc_new");
 	if (bio == NULL) {
 		bio = mpage_alloc(bdev, blocks[0] << (blkbits - 9),
 			  	min_t(int, nr_pages, bio_get_nr_vecs(bdev)),
@@ -307,9 +311,11 @@ alloc_new:
 	else
 		*last_block_in_bio = blocks[blocks_per_page - 1];
 out:
+   TRACING_TAP(read, "do_mpage_readpage stop");
 	return bio;
 
 confused:
+   TRACING_TAP(read, "do_mpage_readpage confused");
 	if (bio)
 		bio = mpage_bio_submit(READ, bio);
 	if (!PageUptodate(page))
diff --git a/fs/read_write.c b/fs/read_write.c
index d065348..859a8b8 100644
--- a/fs/read_write.c
+++ b/fs/read_write.c
@@ -20,6 +20,8 @@
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
 
+#include <trace/events/io-bench.h>
+
 const struct file_operations generic_ro_fops = {
 	.llseek		= generic_file_llseek,
 	.read		= do_sync_read,
@@ -333,6 +335,7 @@ ssize_t do_sync_read(struct file *filp, char __user *buf, size_t len, loff_t *pp
 	struct iovec iov = { .iov_base = buf, .iov_len = len };
 	struct kiocb kiocb;
 	ssize_t ret;
+   TRACING_TAP(read, "do_sync_read start");
 
 	init_sync_kiocb(&kiocb, filp);
 	kiocb.ki_pos = *ppos;
@@ -349,6 +352,7 @@ ssize_t do_sync_read(struct file *filp, char __user *buf, size_t len, loff_t *pp
 	if (-EIOCBQUEUED == ret)
 		ret = wait_on_sync_kiocb(&kiocb);
 	*ppos = kiocb.ki_pos;
+   TRACING_TAP(read, "do_sync_read stop");
 	return ret;
 }
 
@@ -365,6 +369,7 @@ ssize_t vfs_read(struct file *file, char __user *buf, size_t count, loff_t *pos)
 	if (unlikely(!access_ok(VERIFY_WRITE, buf, count)))
 		return -EFAULT;
 
+   TRACING_TAP(read, "vfs_read start");
 	ret = rw_verify_area(READ, file, pos, count);
 	if (ret >= 0) {
 		count = ret;
@@ -389,6 +394,7 @@ ssize_t do_sync_write(struct file *filp, const char __user *buf, size_t len, lof
 	struct iovec iov = { .iov_base = (void __user *)buf, .iov_len = len };
 	struct kiocb kiocb;
 	ssize_t ret;
+   TRACING_TAP(write, "do_sync_write start");
 
 	init_sync_kiocb(&kiocb, filp);
 	kiocb.ki_pos = *ppos;
@@ -405,6 +411,7 @@ ssize_t do_sync_write(struct file *filp, const char __user *buf, size_t len, lof
 	if (-EIOCBQUEUED == ret)
 		ret = wait_on_sync_kiocb(&kiocb);
 	*ppos = kiocb.ki_pos;
+   TRACING_TAP(write, "do_sync_write stop");
 	return ret;
 }
 
@@ -421,6 +428,7 @@ ssize_t vfs_write(struct file *file, const char __user *buf, size_t count, loff_
 	if (unlikely(!access_ok(VERIFY_READ, buf, count)))
 		return -EFAULT;
 
+   TRACING_TAP(write, "vfs_write start");
 	ret = rw_verify_area(WRITE, file, pos, count);
 	if (ret >= 0) {
 		count = ret;
diff --git a/include/trace/events/io-bench-custom-traces.h b/include/trace/events/io-bench-custom-traces.h
new file mode 100644
index 0000000..c483c58
--- /dev/null
+++ b/include/trace/events/io-bench-custom-traces.h
@@ -0,0 +1,2 @@
+TRACING(read)
+TRACING(write)
diff --git a/include/trace/events/io-bench.h b/include/trace/events/io-bench.h
new file mode 100644
index 0000000..fa5a249
--- /dev/null
+++ b/include/trace/events/io-bench.h
@@ -0,0 +1,57 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM io-bench
+
+#if !defined(_TRACING_IO_BENCH_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACING_IO_BENCH_H
+
+#include <linux/tracepoint.h>
+#include <asm/msr.h>
+#include <linux/time.h>
+
+typedef unsigned long long ticks;
+
+#ifdef CREATE_TRACE_POINTS
+#define EXPORT_TRACING(name) EXPORT_TRACEPOINT_SYMBOL(tracing_##name);
+#else
+#define EXPORT_TRACING(name)
+#endif
+
+#define TRACING(tname) \
+TRACE_EVENT(tracing_##tname, \
+   TP_PROTO(const char* extra), \
+   TP_ARGS(extra), \
+   TP_STRUCT__entry( \
+      __field(ticks, tsc) \
+      __field(s64, nanosec) \
+      __field(s64, monotonicnanosec) \
+      __array(char, extra, 50) \
+   ), \
+   TP_fast_assign( \
+      /* unsigned a, d; \
+      __asm__ volatile("lfence") \
+      __asm__ volatile("rdtsc" : "=a"(a), "=d"(d)); \
+      __entry->time = (((ticks)a) | (((ticks)d) << 32)); */ \
+      struct timeval tv; \
+      struct timespec ts; \
+      do_gettimeofday(&tv); \
+      getrawmonotonic(&ts); \
+      __entry->tsc = native_read_tsc(); \
+      __entry->nanosec = timeval_to_ns(&tv); \
+      __entry->monotonicnanosec = timespec_to_ns(&ts); \
+      strncpy(__entry->extra, extra, 50); \
+   ), \
+   TP_printk("IO-bench " #tname ":%s tsc %llu nanosec %lld monotonic %lld\n", \
+      __entry->extra, __entry->tsc, __entry->nanosec, __entry->monotonicnanosec) \
+); \
+EXPORT_TRACING(tname)
+
+#define TRACING_TAP(tname, arg) trace_tracing_##tname(arg)
+
+// Your traces go to this header
+#include "io-bench-custom-traces.h"
+
+#undef EXPORT_TRACING
+
+#endif /* _TRACING_IO_BENCH_H */
+
+#include <trace/define_trace.h>
diff --git a/kernel/trace/Kconfig b/kernel/trace/Kconfig
index 4cea4f4..212ba3c 100644
--- a/kernel/trace/Kconfig
+++ b/kernel/trace/Kconfig
@@ -516,6 +516,10 @@ config RING_BUFFER_BENCHMARK
 
 	  If unsure, say N.
 
+config IO_BENCH
+	tristate "Tracer from IO benchmarking suite"
+	select GENERIC_TRACER
+
 endif # FTRACE
 
 endif # TRACING_SUPPORT
diff --git a/kernel/trace/Makefile b/kernel/trace/Makefile
index d7e2068..d664606 100644
--- a/kernel/trace/Makefile
+++ b/kernel/trace/Makefile
@@ -60,5 +60,8 @@ obj-$(CONFIG_KGDB_KDB) += trace_kdb.o
 endif
 obj-$(CONFIG_PROBE_EVENTS) += trace_probe.o
 obj-$(CONFIG_UPROBE_EVENT) += trace_uprobe.o
+ifeq ($(CONFIG_TRACING),y)
+obj-$(CONFIG_IO_BENCH) += io-bench.o
+endif
 
 libftrace-y := ftrace.o
diff --git a/kernel/trace/io-bench.c b/kernel/trace/io-bench.c
new file mode 100644
index 0000000..7991037
--- /dev/null
+++ b/kernel/trace/io-bench.c
@@ -0,0 +1,18 @@
+#include <linux/module.h>
+
+#define CREATE_TRACE_POINTS
+#include <trace/events/io-bench.h>
+
+static int __init io_bench_init(void)
+{
+   return 0;
+}
+
+static void __exit io_bench_exit(void)
+{
+   return;
+}
+
+module_init(io_bench_init);
+module_exit(io_bench_exit);
+MODULE_LICENSE("GPL");
diff --git a/mm/filemap.c b/mm/filemap.c
index 83efee7..c1712f4 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -42,6 +42,8 @@
 
 #include <asm/mman.h>
 
+#include <trace/events/io-bench.h>
+
 /*
  * Shared mappings implemented 30.11.1994. It's not fully working yet,
  * though.
@@ -1090,6 +1092,7 @@ static void do_generic_file_read(struct file *filp, loff_t *ppos,
 	unsigned long offset;      /* offset into pagecache page */
 	unsigned int prev_offset;
 	int error;
+   TRACING_TAP(read, "do_generic_file_read start");
 
 	index = *ppos >> PAGE_CACHE_SHIFT;
 	prev_index = ra->prev_pos >> PAGE_CACHE_SHIFT;
@@ -1102,9 +1105,11 @@ static void do_generic_file_read(struct file *filp, loff_t *ppos,
 		pgoff_t end_index;
 		loff_t isize;
 		unsigned long nr, ret;
+      TRACING_TAP(read, "do_generic_file_read page loop");
 
 		cond_resched();
 find_page:
+      TRACING_TAP(read, "do_generic_file_read find_page");
 		page = find_get_page(mapping, index);
 		if (!page) {
 			page_cache_sync_readahead(mapping,
@@ -1134,6 +1139,7 @@ find_page:
 			unlock_page(page);
 		}
 page_ok:
+      TRACING_TAP(read, "do_generic_file_read page_ok");
 		/*
 		 * i_size must be checked after we know the page is Uptodate.
 		 *
@@ -1198,12 +1204,14 @@ page_ok:
 		goto out;
 
 page_not_up_to_date:
+      TRACING_TAP(read, "do_generic_file_read page_not_up_to_date");
 		/* Get exclusive access to the page ... */
 		error = lock_page_killable(page);
 		if (unlikely(error))
 			goto readpage_error;
 
 page_not_up_to_date_locked:
+      TRACING_TAP(read, "do_generic_file_read page_not_up_to_date_locked");
 		/* Did it get truncated before we got the lock? */
 		if (!page->mapping) {
 			unlock_page(page);
@@ -1218,6 +1226,7 @@ page_not_up_to_date_locked:
 		}
 
 readpage:
+      TRACING_TAP(read, "do_generic_file_read readpage");
 		/*
 		 * A previous I/O error may have been due to temporary
 		 * failures, eg. multipath errors.
@@ -1259,12 +1268,14 @@ readpage:
 		goto page_ok;
 
 readpage_error:
+      TRACING_TAP(read, "do_generic_file_read readpage_error");
 		/* UHHUH! A synchronous read error occurred. Report it */
 		desc->error = error;
 		page_cache_release(page);
 		goto out;
 
 no_cached_page:
+      TRACING_TAP(read, "do_generic_file_read no_cached_page");
 		/*
 		 * Ok, it wasn't cached, so we need to create a new
 		 * page..
@@ -1287,6 +1298,7 @@ no_cached_page:
 	}
 
 out:
+   TRACING_TAP(read, "do_generic_file_read stop");
 	ra->prev_pos = prev_index;
 	ra->prev_pos <<= PAGE_CACHE_SHIFT;
 	ra->prev_pos |= prev_offset;
@@ -1300,6 +1312,7 @@ int file_read_actor(read_descriptor_t *desc, struct page *page,
 {
 	char *kaddr;
 	unsigned long left, count = desc->count;
+   TRACING_TAP(read, "file_read_actor start");
 
 	if (size > count)
 		size = count;
@@ -1309,6 +1322,7 @@ int file_read_actor(read_descriptor_t *desc, struct page *page,
 	 * taking the kmap.
 	 */
 	if (!fault_in_pages_writeable(desc->arg.buf, size)) {
+      TRACING_TAP(read, "file_read_actor trying kmap_atomic");
 		kaddr = kmap_atomic(page);
 		left = __copy_to_user_inatomic(desc->arg.buf,
 						kaddr + offset, size);
@@ -1317,6 +1331,7 @@ int file_read_actor(read_descriptor_t *desc, struct page *page,
 			goto success;
 	}
 
+   TRACING_TAP(read, "file_read_actor doing kmap");
 	/* Do it the slow way */
 	kaddr = kmap(page);
 	left = __copy_to_user(desc->arg.buf, kaddr + offset, size);
@@ -1327,6 +1342,7 @@ int file_read_actor(read_descriptor_t *desc, struct page *page,
 		desc->error = -EFAULT;
 	}
 success:
+   TRACING_TAP(read, "file_read_actor stop");
 	desc->count = count - size;
 	desc->written += size;
 	desc->arg.buf += size;
@@ -1392,6 +1408,8 @@ generic_file_aio_read(struct kiocb *iocb, const struct iovec *iov,
 	size_t count;
 	loff_t *ppos = &iocb->ki_pos;
 
+   TRACING_TAP(read, "generic_file_aio_read start");
+
 	count = 0;
 	retval = generic_segment_checks(iov, &nr_segs, &count, VERIFY_WRITE);
 	if (retval)
@@ -1412,6 +1430,7 @@ generic_file_aio_read(struct kiocb *iocb, const struct iovec *iov,
 			retval = filemap_write_and_wait_range(mapping, pos,
 					pos + iov_length(iov, nr_segs) - 1);
 			if (!retval) {
+            TRACING_TAP(read, "generic_file_aio_read going direct");
 				retval = mapping->a_ops->direct_IO(READ, iocb,
 							iov, pos, nr_segs);
 			}
@@ -1436,6 +1455,7 @@ generic_file_aio_read(struct kiocb *iocb, const struct iovec *iov,
 	}
 
 	count = retval;
+   TRACING_TAP(read, "generic_file_aio_read going regular");
 	for (seg = 0; seg < nr_segs; seg++) {
 		read_descriptor_t desc;
 		loff_t offset = 0;
@@ -1469,6 +1489,7 @@ generic_file_aio_read(struct kiocb *iocb, const struct iovec *iov,
 			break;
 	}
 out:
+   TRACING_TAP(read, "generic_file_aio_read stop");
 	return retval;
 }
 EXPORT_SYMBOL(generic_file_aio_read);
@@ -2287,6 +2308,7 @@ static ssize_t generic_perform_write(struct file *file,
 	long status = 0;
 	ssize_t written = 0;
 	unsigned int flags = 0;
+   TRACING_TAP(write, "generic_perform_write start");
 
 	/*
 	 * Copies from kernel address space cannot fail (NFSD is a big user).
@@ -2300,12 +2322,14 @@ static ssize_t generic_perform_write(struct file *file,
 		unsigned long bytes;	/* Bytes to write to page */
 		size_t copied;		/* Bytes copied from user */
 		void *fsdata;
+      TRACING_TAP(write, "generic_perform_write loop start");
 
 		offset = (pos & (PAGE_CACHE_SIZE - 1));
 		bytes = min_t(unsigned long, PAGE_CACHE_SIZE - offset,
 						iov_iter_count(i));
 
 again:
+      TRACING_TAP(write, "generic_perform_write again");
 		/*
 		 * Bring in the user page that we will copy from _first_.
 		 * Otherwise there's a nasty deadlock on copying from the
@@ -2366,6 +2390,7 @@ again:
 			break;
 		}
 	} while (iov_iter_count(i));
+   TRACING_TAP(write, "generic_perform_write stop");
 
 	return written ? written : status;
 }
@@ -2421,6 +2446,7 @@ ssize_t __generic_file_aio_write(struct kiocb *iocb, const struct iovec *iov,
 	loff_t		pos;
 	ssize_t		written;
 	ssize_t		err;
+   TRACING_TAP(write, "__generic_file_aio_write start");
 
 	ocount = 0;
 	err = generic_segment_checks(iov, &nr_segs, &ocount, VERIFY_READ);
@@ -2453,6 +2479,7 @@ ssize_t __generic_file_aio_write(struct kiocb *iocb, const struct iovec *iov,
 	if (unlikely(file->f_flags & O_DIRECT)) {
 		loff_t endbyte;
 		ssize_t written_buffered;
+      TRACING_TAP(write, "__generic_file_aio_write going direct");
 
 		written = generic_file_direct_write(iocb, iov, &nr_segs, pos,
 							ppos, count, ocount);
@@ -2498,10 +2525,12 @@ ssize_t __generic_file_aio_write(struct kiocb *iocb, const struct iovec *iov,
 			 */
 		}
 	} else {
+      TRACING_TAP(write, "__generic_file_aio_write going buffered");
 		written = generic_file_buffered_write(iocb, iov, nr_segs,
 				pos, ppos, count, written);
 	}
 out:
+   TRACING_TAP(write, "__generic_file_aio_write stop");
 	current->backing_dev_info = NULL;
 	return written ? written : err;
 }
@@ -2526,6 +2555,7 @@ ssize_t generic_file_aio_write(struct kiocb *iocb, const struct iovec *iov,
 	ssize_t ret;
 
 	BUG_ON(iocb->ki_pos != pos);
+   TRACING_TAP(write, "generic_file_aio_write start");
 
 	sb_start_write(inode->i_sb);
 	mutex_lock(&inode->i_mutex);
@@ -2535,11 +2565,13 @@ ssize_t generic_file_aio_write(struct kiocb *iocb, const struct iovec *iov,
 	if (ret > 0 || ret == -EIOCBQUEUED) {
 		ssize_t err;
 
+      TRACING_TAP(write, "generic_file_aio_write synchronisation");
 		err = generic_write_sync(file, pos, ret);
 		if (err < 0 && ret > 0)
 			ret = err;
 	}
 	sb_end_write(inode->i_sb);
+   TRACING_TAP(write, "generic_file_aio_write stop");
 	return ret;
 }
 EXPORT_SYMBOL(generic_file_aio_write);
diff --git a/mm/readahead.c b/mm/readahead.c
index 7963f23..41f2fa2 100644
--- a/mm/readahead.c
+++ b/mm/readahead.c
@@ -19,6 +19,7 @@
 #include <linux/pagemap.h>
 #include <linux/syscalls.h>
 #include <linux/file.h>
+#include <trace/events/io-bench.h>
 
 /*
  * Initialise a struct file's readahead state.  Assumes that the caller has
@@ -114,6 +115,7 @@ static int read_pages(struct address_space *mapping, struct file *filp,
 	struct blk_plug plug;
 	unsigned page_idx;
 	int ret;
+   TRACING_TAP(read, "read_pages start");
 
 	blk_start_plug(&plug);
 
@@ -136,6 +138,7 @@ static int read_pages(struct address_space *mapping, struct file *filp,
 	ret = 0;
 
 out:
+   TRACING_TAP(read, "read_pages stop");
 	blk_finish_plug(&plug);
 
 	return ret;
-- 
1.8.2.1

